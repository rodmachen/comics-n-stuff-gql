#!/usr/bin/env python3
"""
Migrate DC Comics data from MySQL dump to local Postgres.
Filters to publisher_id=54 (DC) and only loads needed tables.
Writes a Postgres-compatible SQL file, then loads it in one shot.
"""

import re
import subprocess
import sys
import os
import tempfile

DUMP_FILE = "2026-02-15.sql"
DB_NAME = "comics_gcd"
DC_PUBLISHER_ID = 54
PSQL = "/opt/homebrew/opt/postgresql@17/bin/psql"
OUTPUT_FILE = "scripts/dc-comics-postgres.sql"

# Tables to load fully (small reference/lookup tables)
FULL_TABLES = {
    "stddata_country",
    "stddata_language",
    "gcd_credit_type",
    "gcd_story_type",
    "gcd_series_publication_type",
}

# Tables to load with DC filtering
FILTERED_TABLES = [
    "gcd_publisher",
    "gcd_series",
    "gcd_issue",
    "gcd_story",
    "gcd_story_credit",
    "gcd_creator_name_detail",
    "gcd_creator",
]

ALL_TABLES = FULL_TABLES | set(FILTERED_TABLES)


def mysql_to_postgres_ddl(ddl: str, table_name: str) -> str:
    """Convert a MySQL CREATE TABLE statement to Postgres, line by line."""
    # Remove MySQL-specific comments
    ddl = re.sub(r'/\*!.*?\*/', '', ddl)

    lines = ddl.split("\n")
    # Collect column/constraint definition lines (inside the parens)
    create_line = None
    col_lines = []
    for line in lines:
        stripped = line.strip()

        # The CREATE TABLE line
        if stripped.startswith('CREATE TABLE'):
            create_line = line.replace('`', '"')
            continue

        # Skip the ENGINE/closing line
        if stripped.startswith(')'):
            continue

        # Skip KEY lines (indexes)
        if re.match(r'KEY\s+`', stripped):
            continue
        # Skip CONSTRAINT/FOREIGN KEY lines
        if 'CONSTRAINT' in stripped and 'FOREIGN KEY' in stripped:
            continue

        # Replace backticks with double quotes
        line = line.replace('`', '"')

        # Type conversions (order matters)
        line = re.sub(r'tinyint\(1\)', 'smallint', line, flags=re.IGNORECASE)
        line = re.sub(r'\bint\s+NOT\s+NULL\s+AUTO_INCREMENT', 'integer GENERATED BY DEFAULT AS IDENTITY', line, flags=re.IGNORECASE)
        line = re.sub(r'\bint\(\d+\)', 'integer', line, flags=re.IGNORECASE)
        line = re.sub(r'\bint\b', 'integer', line, flags=re.IGNORECASE)
        line = re.sub(r'\blongtext\b', 'text', line, flags=re.IGNORECASE)
        line = re.sub(r'\bdatetime\(\d+\)', 'timestamp', line, flags=re.IGNORECASE)
        line = re.sub(r'\bdatetime\b', 'timestamp', line, flags=re.IGNORECASE)
        line = re.sub(r'UNIQUE KEY "[^"]*"', 'UNIQUE', line)

        # Only keep non-empty lines
        if stripped:
            col_lines.append(line)

    # Remove trailing commas from the last column line
    if col_lines:
        col_lines[-1] = col_lines[-1].rstrip().rstrip(',')

    result = create_line + "\n"
    result += "\n".join(col_lines) + "\n"
    result += ");\n"

    return result


def get_column_index(ddl: str, column_name: str) -> int:
    """Find the 0-based column index from a CREATE TABLE DDL."""
    col_idx = 0
    for line in ddl.split("\n"):
        line = line.strip()
        m = re.match(r'`(\w+)`', line)
        if m:
            if m.group(1) == column_name:
                return col_idx
            col_idx += 1
    return -1


def parse_values_from_insert(line: str):
    """
    Parse MySQL INSERT INTO `table` VALUES (...),(...),... ;
    Returns list of raw row strings (contents between parens).
    """
    m = re.match(r"INSERT INTO `[^`]+` VALUES\s*", line)
    if not m:
        return []

    data = line[m.end():]
    rows = []
    i = 0
    n = len(data)

    while i < n:
        if data[i] == '(':
            depth = 1
            i += 1
            start = i
            in_quote = False
            escape_next = False
            while i < n and depth > 0:
                c = data[i]
                if escape_next:
                    escape_next = False
                elif c == '\\':
                    escape_next = True
                elif c == "'" and not in_quote:
                    in_quote = True
                elif c == "'" and in_quote:
                    in_quote = False
                elif not in_quote:
                    if c == '(':
                        depth += 1
                    elif c == ')':
                        depth -= 1
                i += 1
            rows.append(data[start:i - 1])
        else:
            i += 1

    return rows


def split_row_fields(row_str: str):
    """Split a row's value string into individual field values."""
    fields = []
    i = 0
    n = len(row_str)

    while i < n:
        if row_str[i] == "'":
            # Quoted string - find the end
            i += 1
            start = i - 1  # include opening quote
            while i < n:
                if row_str[i] == '\\':
                    i += 2  # skip escaped char
                elif row_str[i] == "'":
                    i += 1
                    break
                else:
                    i += 1
            fields.append(row_str[start:i])
        elif row_str[i] == ',':
            i += 1
        elif row_str[i] in (' ', '\t'):
            i += 1
        else:
            start = i
            while i < n and row_str[i] != ',':
                i += 1
            fields.append(row_str[start:i].strip())

    return fields


def mysql_val_to_postgres(val: str) -> str:
    """Convert a single MySQL value to Postgres format."""
    if val == 'NULL':
        return 'NULL'
    if val.startswith("'") and val.endswith("'"):
        inner = val[1:-1]
        # Convert MySQL backslash escapes to Postgres
        # We need E'' string syntax for backslash escapes
        # First unescape MySQL, then re-escape for Postgres E'' strings
        result = []
        i = 0
        needs_e = False
        while i < len(inner):
            if inner[i] == '\\' and i + 1 < len(inner):
                nc = inner[i + 1]
                if nc == "'":
                    result.append("'")
                elif nc == '"':
                    result.append('"')
                elif nc == 'n':
                    result.append('\n')
                    needs_e = True
                elif nc == 'r':
                    result.append('\r')
                    needs_e = True
                elif nc == 't':
                    result.append('\t')
                    needs_e = True
                elif nc == '\\':
                    result.append('\\')
                    needs_e = True
                elif nc == '0':
                    result.append('\0')
                    needs_e = True
                else:
                    result.append(nc)
                i += 2
            else:
                result.append(inner[i])
                i += 1

        text = ''.join(result)
        # Escape for Postgres
        if needs_e:
            text = text.replace('\\', '\\\\')
            text = text.replace("'", "''")
            text = text.replace('\n', '\\n')
            text = text.replace('\r', '\\r')
            text = text.replace('\t', '\\t')
            text = text.replace('\0', '')
            return "E'" + text + "'"
        else:
            text = text.replace("'", "''")
            return "'" + text + "'"
    return val


def row_to_postgres_values(row_str: str) -> str:
    """Convert a full MySQL VALUES row to Postgres values string."""
    fields = split_row_fields(row_str)
    pg_fields = [mysql_val_to_postgres(f) for f in fields]
    return "(" + ",".join(pg_fields) + ")"


def main():
    print("=== DC Comics Data Migration: MySQL -> Postgres ===\n")

    # Phase 1: Parse the dump file
    print("Phase 1: Parsing MySQL dump file...")
    print(f"  File: {DUMP_FILE}")

    current_table = None
    ddl_blocks = {}
    ddl_buffer = []
    in_ddl = False
    insert_lines = {}

    with open(DUMP_FILE, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            if line_num % 1000 == 0:
                print(f"  Line {line_num}...", end="\r")

            m = re.match(r"CREATE TABLE `(\w+)`", line)
            if m:
                tname = m.group(1)
                if tname in ALL_TABLES:
                    current_table = tname
                    ddl_buffer = [line]
                    in_ddl = True
                continue

            if in_ddl:
                ddl_buffer.append(line)
                if re.match(r'\)', line):
                    ddl_blocks[current_table] = "".join(ddl_buffer)
                    in_ddl = False
                    current_table = None
                continue

            m2 = re.match(r"INSERT INTO `(\w+)`", line)
            if m2:
                tname = m2.group(1)
                if tname in ALL_TABLES:
                    if tname not in insert_lines:
                        insert_lines[tname] = []
                    insert_lines[tname].append(line)

    print(f"\n  Found DDL for {len(ddl_blocks)} tables")
    print(f"  Found INSERT data for {len(insert_lines)} tables")

    # Phase 2: Identify column indices for filtering
    print("\nPhase 2: Identifying filter columns...")
    series_pub_idx = get_column_index(ddl_blocks.get("gcd_series", ""), "publisher_id")
    issue_series_idx = get_column_index(ddl_blocks.get("gcd_issue", ""), "series_id")
    story_issue_idx = get_column_index(ddl_blocks.get("gcd_story", ""), "issue_id")
    credit_story_idx = get_column_index(ddl_blocks.get("gcd_story_credit", ""), "story_id")
    credit_creator_idx = get_column_index(ddl_blocks.get("gcd_story_credit", ""), "creator_id")
    namedetail_creator_idx = get_column_index(ddl_blocks.get("gcd_creator_name_detail", ""), "creator_id")

    print(f"  gcd_series.publisher_id at index {series_pub_idx}")
    print(f"  gcd_issue.series_id at index {issue_series_idx}")
    print(f"  gcd_story.issue_id at index {story_issue_idx}")
    print(f"  gcd_story_credit.story_id at index {credit_story_idx}")
    print(f"  gcd_story_credit.creator_id at index {credit_creator_idx}")
    print(f"  gcd_creator_name_detail.creator_id at index {namedetail_creator_idx}")

    # Phase 3: Filter data and collect IDs through the chain
    print("\nPhase 3: Filtering DC data through the chain...")

    # Collect DC series IDs
    dc_series_ids = set()
    print("  Scanning gcd_series for DC (publisher_id=54)...")
    for line in insert_lines.get("gcd_series", []):
        for row_str in parse_values_from_insert(line):
            fields = split_row_fields(row_str)
            if len(fields) > series_pub_idx and fields[series_pub_idx] == str(DC_PUBLISHER_ID):
                dc_series_ids.add(int(fields[0]))
    print(f"    Found {len(dc_series_ids)} DC series")

    # Collect DC issue IDs
    dc_issue_ids = set()
    print("  Scanning gcd_issue for DC series...")
    for line in insert_lines.get("gcd_issue", []):
        for row_str in parse_values_from_insert(line):
            fields = split_row_fields(row_str)
            if len(fields) > issue_series_idx:
                try:
                    if int(fields[issue_series_idx]) in dc_series_ids:
                        dc_issue_ids.add(int(fields[0]))
                except ValueError:
                    pass
    print(f"    Found {len(dc_issue_ids)} DC issues")

    # Collect DC story IDs
    dc_story_ids = set()
    print("  Scanning gcd_story for DC issues...")
    for line in insert_lines.get("gcd_story", []):
        for row_str in parse_values_from_insert(line):
            fields = split_row_fields(row_str)
            if len(fields) > story_issue_idx:
                try:
                    if int(fields[story_issue_idx]) in dc_issue_ids:
                        dc_story_ids.add(int(fields[0]))
                except ValueError:
                    pass
    print(f"    Found {len(dc_story_ids)} DC stories")

    # Collect DC story credit creator_name_detail IDs
    dc_credit_rows = []
    creator_name_detail_ids = set()
    print("  Scanning gcd_story_credit for DC stories...")
    for line in insert_lines.get("gcd_story_credit", []):
        for row_str in parse_values_from_insert(line):
            fields = split_row_fields(row_str)
            if len(fields) > credit_story_idx:
                try:
                    if int(fields[credit_story_idx]) in dc_story_ids:
                        dc_credit_rows.append(row_str)
                        if len(fields) > credit_creator_idx:
                            try:
                                creator_name_detail_ids.add(int(fields[credit_creator_idx]))
                            except ValueError:
                                pass
                except ValueError:
                    pass
    print(f"    Found {len(dc_credit_rows)} DC story credits")
    print(f"    Referencing {len(creator_name_detail_ids)} creator name details")

    # Collect creator IDs from name details
    dc_namedetail_rows = []
    creator_ids = set()
    print("  Scanning gcd_creator_name_detail for referenced creators...")
    for line in insert_lines.get("gcd_creator_name_detail", []):
        for row_str in parse_values_from_insert(line):
            fields = split_row_fields(row_str)
            try:
                if int(fields[0]) in creator_name_detail_ids:
                    dc_namedetail_rows.append(row_str)
                    if len(fields) > namedetail_creator_idx:
                        try:
                            creator_ids.add(int(fields[namedetail_creator_idx]))
                        except ValueError:
                            pass
            except (ValueError, IndexError):
                pass
    print(f"    Found {len(dc_namedetail_rows)} creator name details")
    print(f"    Referencing {len(creator_ids)} unique creators")

    # Phase 4: Write Postgres SQL file
    print(f"\nPhase 4: Writing Postgres SQL to {OUTPUT_FILE}...")

    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)

    with open(OUTPUT_FILE, "w", encoding="utf-8") as out:
        out.write("-- DC Comics data migrated from GCD MySQL dump\n")
        out.write("-- Auto-generated by migrate-to-postgres.py\n\n")
        out.write("BEGIN;\n\n")

        # DDL for all tables
        table_order = [
            "stddata_country", "stddata_language",
            "gcd_credit_type", "gcd_story_type", "gcd_series_publication_type",
            "gcd_publisher", "gcd_series", "gcd_issue", "gcd_story",
            "gcd_story_credit", "gcd_creator", "gcd_creator_name_detail",
        ]

        for tname in table_order:
            if tname in ddl_blocks:
                out.write(f'DROP TABLE IF EXISTS "{tname}" CASCADE;\n')
                pg_ddl = mysql_to_postgres_ddl(ddl_blocks[tname], tname)
                out.write(pg_ddl)
                out.write("\n\n")

        # Data: full reference tables
        for tname in ["stddata_country", "stddata_language", "gcd_credit_type",
                       "gcd_story_type", "gcd_series_publication_type"]:
            if tname in insert_lines:
                out.write(f"-- Data for {tname}\n")
                count = 0
                for line in insert_lines[tname]:
                    rows = parse_values_from_insert(line)
                    if rows:
                        batch_size = 500
                        for bi in range(0, len(rows), batch_size):
                            batch = rows[bi:bi + batch_size]
                            pg_vals = [row_to_postgres_values(r) for r in batch]
                            out.write(f'INSERT INTO "{tname}" VALUES\n')
                            out.write(",\n".join(pg_vals))
                            out.write(";\n")
                            count += len(batch)
                print(f"  {tname}: {count} rows")

        # Data: gcd_publisher (DC only)
        out.write(f"\n-- Data for gcd_publisher (DC only)\n")
        for line in insert_lines.get("gcd_publisher", []):
            for row_str in parse_values_from_insert(line):
                fields = split_row_fields(row_str)
                if fields and fields[0] == str(DC_PUBLISHER_ID):
                    out.write(f'INSERT INTO "gcd_publisher" VALUES\n')
                    out.write(row_to_postgres_values(row_str))
                    out.write(";\n")
                    print(f"  gcd_publisher: 1 row (DC)")
                    break

        # Data: gcd_series (DC only)
        out.write(f"\n-- Data for gcd_series (DC only)\n")
        count = 0
        for line in insert_lines.get("gcd_series", []):
            rows = parse_values_from_insert(line)
            dc_rows = []
            for row_str in rows:
                fields = split_row_fields(row_str)
                if len(fields) > series_pub_idx and fields[series_pub_idx] == str(DC_PUBLISHER_ID):
                    dc_rows.append(row_str)
            if dc_rows:
                for bi in range(0, len(dc_rows), 500):
                    batch = dc_rows[bi:bi + 500]
                    pg_vals = [row_to_postgres_values(r) for r in batch]
                    out.write(f'INSERT INTO "gcd_series" VALUES\n')
                    out.write(",\n".join(pg_vals))
                    out.write(";\n")
                    count += len(batch)
        print(f"  gcd_series: {count} rows")

        # Data: gcd_issue (DC only)
        out.write(f"\n-- Data for gcd_issue (DC only)\n")
        count = 0
        for line in insert_lines.get("gcd_issue", []):
            rows = parse_values_from_insert(line)
            dc_rows = []
            for row_str in rows:
                fields = split_row_fields(row_str)
                if len(fields) > issue_series_idx:
                    try:
                        if int(fields[issue_series_idx]) in dc_series_ids:
                            dc_rows.append(row_str)
                    except ValueError:
                        pass
            if dc_rows:
                for bi in range(0, len(dc_rows), 500):
                    batch = dc_rows[bi:bi + 500]
                    pg_vals = [row_to_postgres_values(r) for r in batch]
                    out.write(f'INSERT INTO "gcd_issue" VALUES\n')
                    out.write(",\n".join(pg_vals))
                    out.write(";\n")
                    count += len(batch)
        print(f"  gcd_issue: {count} rows")

        # Data: gcd_story (DC only)
        out.write(f"\n-- Data for gcd_story (DC only)\n")
        count = 0
        for line in insert_lines.get("gcd_story", []):
            rows = parse_values_from_insert(line)
            dc_rows = []
            for row_str in rows:
                fields = split_row_fields(row_str)
                if len(fields) > story_issue_idx:
                    try:
                        if int(fields[story_issue_idx]) in dc_issue_ids:
                            dc_rows.append(row_str)
                    except ValueError:
                        pass
            if dc_rows:
                for bi in range(0, len(dc_rows), 500):
                    batch = dc_rows[bi:bi + 500]
                    pg_vals = [row_to_postgres_values(r) for r in batch]
                    out.write(f'INSERT INTO "gcd_story" VALUES\n')
                    out.write(",\n".join(pg_vals))
                    out.write(";\n")
                    count += len(batch)
        print(f"  gcd_story: {count} rows")

        # Data: gcd_creator (referenced by DC credits)
        # Load creators BEFORE name_details due to potential FK
        out.write(f"\n-- Data for gcd_creator (referenced by DC credits)\n")
        count = 0
        for line in insert_lines.get("gcd_creator", []):
            rows = parse_values_from_insert(line)
            matching = []
            for row_str in rows:
                fields = split_row_fields(row_str)
                try:
                    if int(fields[0]) in creator_ids:
                        matching.append(row_str)
                except (ValueError, IndexError):
                    pass
            if matching:
                for bi in range(0, len(matching), 500):
                    batch = matching[bi:bi + 500]
                    pg_vals = [row_to_postgres_values(r) for r in batch]
                    out.write(f'INSERT INTO "gcd_creator" VALUES\n')
                    out.write(",\n".join(pg_vals))
                    out.write(";\n")
                    count += len(batch)
        print(f"  gcd_creator: {count} rows")

        # Data: gcd_creator_name_detail
        out.write(f"\n-- Data for gcd_creator_name_detail (referenced by DC credits)\n")
        if dc_namedetail_rows:
            for bi in range(0, len(dc_namedetail_rows), 500):
                batch = dc_namedetail_rows[bi:bi + 500]
                pg_vals = [row_to_postgres_values(r) for r in batch]
                out.write(f'INSERT INTO "gcd_creator_name_detail" VALUES\n')
                out.write(",\n".join(pg_vals))
                out.write(";\n")
        print(f"  gcd_creator_name_detail: {len(dc_namedetail_rows)} rows")

        # Data: gcd_story_credit
        out.write(f"\n-- Data for gcd_story_credit (DC stories only)\n")
        if dc_credit_rows:
            for bi in range(0, len(dc_credit_rows), 500):
                batch = dc_credit_rows[bi:bi + 500]
                pg_vals = [row_to_postgres_values(r) for r in batch]
                out.write(f'INSERT INTO "gcd_story_credit" VALUES\n')
                out.write(",\n".join(pg_vals))
                out.write(";\n")
        print(f"  gcd_story_credit: {len(dc_credit_rows)} rows")

        out.write("\nCOMMIT;\n")

    file_size = os.path.getsize(OUTPUT_FILE)
    print(f"\n  Output file: {OUTPUT_FILE} ({file_size / 1024 / 1024:.1f} MB)")

    # Phase 5: Load into Postgres
    print(f"\nPhase 5: Loading into Postgres ({DB_NAME})...")
    proc = subprocess.run(
        [PSQL, "-d", DB_NAME, "-f", OUTPUT_FILE],
        capture_output=True, text=True,
    )
    if proc.returncode != 0:
        print(f"  ERROR: {proc.stderr[:2000]}")
        # Show first few errors
        for err_line in proc.stderr.split("\n")[:20]:
            if err_line.strip():
                print(f"  {err_line}")
    else:
        print("  Loaded successfully!")

    # Phase 6: Verify
    print("\nPhase 6: Verification...")
    for tname in table_order:
        proc = subprocess.run(
            [PSQL, "-d", DB_NAME, "-t", "-c", f'SELECT COUNT(*) FROM "{tname}";'],
            capture_output=True, text=True,
        )
        cnt = proc.stdout.strip() if proc.returncode == 0 else "ERROR"
        print(f"  {tname}: {cnt} rows")

    print("\nDone!")


if __name__ == "__main__":
    main()
